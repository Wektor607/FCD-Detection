/home/s17gmikh/miniconda3/envs/MELD-env/bin/python
/home/s17gmikh/miniconda3/envs/MELD-env/bin/python3
cuda: True
256 128 768 256
256 64 768 128
256 64 768 128
2025-05-27 11:29:12,643 - GPU available: True (cuda), used: True
2025-05-27 11:29:12,645 - TPU available: False, using: 0 TPU cores
2025-05-27 11:29:12,645 - IPU available: False, using: 0 IPUs
2025-05-27 11:29:12,645 - HPU available: False, using: 0 HPUs
start training
2025-05-27 11:29:12,652 - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
2025-05-27 11:29:14,751 - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2025-05-27 11:29:14,761 - You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
2025-05-27 11:29:16,064 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Training is not really distributed, single rank. Deactivating buckets
ShardedDDP bucket size: 0.00M parameters, model size 110.51M parameters
2025-05-27 11:29:16,528 - 
  | Name          | Type           | Params
-------------------------------------------------
0 | model         | LanGuideMedSeg | 115 M 
1 | loss_fn       | DiceCELoss     | 0     
2 | train_metrics | ModuleDict     | 0     
3 | val_metrics   | ModuleDict     | 0     
4 | test_metrics  | ModuleDict     | 0     
-------------------------------------------------
6.2 M     Trainable params
109 M     Non-trainable params
115 M     Total params
463.497   Total estimated model params size (MB)
2025-05-27 11:29:16,540 - SLURM auto-requeueing enabled. Setting signal handlers.
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:29:23
{'epoch': 0, 'val_loss': 0.9988805055618286, 'val_acc': 0.0005594889516942203, 'val_dice': 0.0011183521710336208, 'val_MIoU': 0.0005594889516942203}
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:29:33
{'epoch': 0, 'val_loss': 0.998827338218689, 'val_acc': 0.0005594889516942203, 'val_dice': 0.0011183521710336208, 'val_MIoU': 0.0005594889516942203}
{'epoch': 0, 'train_loss': 0.9994462132453918, 'train_acc': 0.0567045658826828, 'train_dice': 0.0005382953677326441, 'train_MIoU': 0.00026922012330032885}
2025-05-27 11:29:33,931 - Epoch 0, global step 2: 'val_loss' reached 0.99883 (best 0.99883), saving model to './save_model/medseg-v6.ckpt' as top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:29:52
{'epoch': 1, 'val_loss': 0.9987590312957764, 'val_acc': 0.0006641206564381719, 'val_dice': 0.001118469168432057, 'val_MIoU': 0.0005595474503934383}
{'epoch': 1, 'train_loss': 0.9995585083961487, 'train_acc': 0.36210647225379944, 'train_dice': 0.0005355342291295528, 'train_MIoU': 0.00026783885550685227}
2025-05-27 11:29:52,768 - Epoch 1, global step 4: 'val_loss' reached 0.99876 (best 0.99876), saving model to './save_model/medseg-v6.ckpt' as top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:10
{'epoch': 2, 'val_loss': 0.9988076686859131, 'val_acc': 0.0020490374881774187, 'val_dice': 0.001120019587688148, 'val_MIoU': 0.0005603235331363976}
{'epoch': 2, 'train_loss': 0.999279797077179, 'train_acc': 0.50714111328125, 'train_dice': 0.0007519036298617721, 'train_MIoU': 0.00037609320133924484}
2025-05-27 11:30:10,938 - Epoch 2, global step 6: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:19
{'epoch': 3, 'val_loss': 0.9988424181938171, 'val_acc': 0.0073082335293293, 'val_dice': 0.0011259466409683228, 'val_MIoU': 0.000563290435820818}
{'epoch': 3, 'train_loss': 0.9992122650146484, 'train_acc': 0.5901913642883301, 'train_dice': 0.0010598653461784124, 'train_MIoU': 0.0005302136996760964}
2025-05-27 11:30:19,512 - Epoch 3, global step 8: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:27
{'epoch': 4, 'val_loss': 0.9988732933998108, 'val_acc': 0.0011378696653991938, 'val_dice': 0.0011189989745616913, 'val_MIoU': 0.000559812702704221}
{'epoch': 4, 'train_loss': 0.9991921186447144, 'train_acc': 0.6267165541648865, 'train_dice': 0.0013094684109091759, 'train_MIoU': 0.0006551631959155202}
2025-05-27 11:30:27,783 - Epoch 4, global step 10: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:34
{'epoch': 5, 'val_loss': 0.9988797307014465, 'val_acc': 0.0006437755655497313, 'val_dice': 0.0011184463510289788, 'val_MIoU': 0.0005595360998995602}
{'epoch': 5, 'train_loss': 0.9991471767425537, 'train_acc': 0.6760846972465515, 'train_dice': 0.001336953486315906, 'train_MIoU': 0.0006689251167699695}
2025-05-27 11:30:35,010 - Epoch 5, global step 12: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:42
{'epoch': 6, 'val_loss': 0.9988808631896973, 'val_acc': 0.0006016322295181453, 'val_dice': 0.0011183993192389607, 'val_MIoU': 0.0005595125257968903}
{'epoch': 6, 'train_loss': 0.9993606805801392, 'train_acc': 0.6474801301956177, 'train_dice': 0.0011266102083027363, 'train_MIoU': 0.0005636226269416511}
2025-05-27 11:30:42,116 - Epoch 6, global step 14: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:49
{'epoch': 7, 'val_loss': 0.9988793730735779, 'val_acc': 0.0008268810342997313, 'val_dice': 0.0011186511255800724, 'val_MIoU': 0.0005596386035904288}
{'epoch': 7, 'train_loss': 0.999314546585083, 'train_acc': 0.644033670425415, 'train_dice': 0.0013926259707659483, 'train_MIoU': 0.0006967981462366879}
2025-05-27 11:30:49,283 - Epoch 7, global step 16: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:30:56
{'epoch': 8, 'val_loss': 0.9988766312599182, 'val_acc': 0.0017409551655873656, 'val_dice': 0.0011196742998436093, 'val_MIoU': 0.0005601507145911455}
{'epoch': 8, 'train_loss': 0.9992693662643433, 'train_acc': 0.6789265871047974, 'train_dice': 0.0010019841138273478, 'train_MIoU': 0.0005012431647628546}
2025-05-27 11:30:56,437 - Epoch 8, global step 18: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:31:03
{'epoch': 9, 'val_loss': 0.9988740086555481, 'val_acc': 0.004855201579630375, 'val_dice': 0.0011231743264943361, 'val_MIoU': 0.00056190270697698}
{'epoch': 9, 'train_loss': 0.9989862442016602, 'train_acc': 0.6906121969223022, 'train_dice': 0.0014633432729169726, 'train_MIoU': 0.0007322073797695339}
2025-05-27 11:31:03,484 - Epoch 9, global step 20: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([4, 256, 1, 6, 7])
torch.Size([4, 128, 2, 12, 14])

torch.Size([4, 128, 2, 12, 14])
torch.Size([4, 128, 4, 24, 28])

torch.Size([4, 128, 4, 24, 28])
torch.Size([4, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([4, 1, 16, 96, 112])

================================================================================2025-05-27 11:31:10
{'epoch': 10, 'val_loss': 0.9988539814949036, 'val_acc': 0.0226614810526371, 'val_dice': 0.0011436141794547439, 'val_MIoU': 0.0005721342167817056}
{'epoch': 10, 'train_loss': 0.9987385272979736, 'train_acc': 0.6705124378204346, 'train_dice': 0.0016345165204256773, 'train_MIoU': 0.0008179267169907689}
2025-05-27 11:31:10,695 - Epoch 10, global step 22: 'val_loss' was not in top 1
Grads waiting to be reduced. If this is on purpose (grad accumulation), please use a no_sync() context
torch.Size([8, 256, 1, 6, 7])
torch.Size([8, 128, 2, 12, 14])

torch.Size([8, 128, 2, 12, 14])
torch.Size([8, 128, 4, 24, 28])

torch.Size([8, 128, 4, 24, 28])
torch.Size([8, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([8, 1, 16, 96, 112])
torch.Size([2, 256, 1, 6, 7])
torch.Size([2, 128, 2, 12, 14])

torch.Size([2, 128, 2, 12, 14])
torch.Size([2, 128, 4, 24, 28])

torch.Size([2, 128, 4, 24, 28])
torch.Size([2, 64, 8, 48, 56])

Out_shape_before_resampling: torch.Size([2, 1, 16, 96, 112])
