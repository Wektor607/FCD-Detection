\documentclass[FCD_GNN.tex]{subfiles}

\begin{document}
\chapter{Conclusion}
\label{chapter:Conclusion}

This thesis introduced a novel multimodal segmentation framework for detecting focal cortical dysplasia type II, combining surface-based visual representations with textual priors derived from anatomical atlases. The main findings are summarized below:

\begin{itemize}
    \item Incorporating textual features consistently improved segmentation performance over vision-only baselines, demonstrating the value of multimodal fusion in low-data medical imaging scenarios.

    \item Aggregating MELD features using a GNN block increased the number of detected lesions but also introduced a higher rate of false positives. While this indicates that additional message passing can enhance sensitivity, it also shows that the current aggregation strategy requires further refinement to avoid degrading overall quality.

    \item Even coarse textual labels such as hemisphere- or lobe-level descriptions yielded substantial improvements. This suggests that fine-grained region names are not strictly necessary, and that lightweight, structured prompts may offer a more robust and generalizable alternative to detailed atlas-style descriptions.
\end{itemize}

Overall, the results demonstrate that multimodal architectures integrating language and surface-based vision are both feasible and beneficial for epileptogenic lesion detection under limited-data conditions. This work represents an initial step toward leveraging anatomical language priors in surface-based segmentation models.

Future research should explore training on larger and more heterogeneous cohorts, incorporating clinical radiology reports as richer textual supervision, and investigating alternative fusion strategies or graph-based aggregation mechanisms to improve robustness, reduce false positives, and enhance generalizability across clinical centers.

\end{document}

