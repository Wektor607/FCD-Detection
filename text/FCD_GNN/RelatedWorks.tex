\documentclass[FCD_GNN.tex]{subfiles}

\begin{document}
\chapter{Related Works}
\label{chapter:Related_works}

This section reviews existing approaches to \ac{fcd} detection and text-guided medical image segmentation for tumor detection 
tasks, focusing on their architecture, fusion strategies, and limitations.

Early work on \ac{fcd} detection focused exclusively on vision-based methods~\cite{Durgam2025LungCancer, Abdusalomov2023BrainTumor, Sharma2025KidneyCT, Mehmood2025BreastCancer}. 
It is noteworthy that one of the newest models, the \ac{meld} Graph~\cite{Ripart2025MELD}, represents the surface of the cerebral cortex as a graph with multiple resolutions and applies a \ac{gnn}-U-Net to segment lesions. It provides high 
sensitivity and specificity by identifying characteristic peaks (more than 20\% in saliency) and determining calibration reliability using the expected 
calibration error. However, it has not been tested on patients with multiple \ac{fcd}s, it lacks cross-attentional mechanisms for more complete integration of 
features, and it does not integrate textual clinical information or evaluate zero-shot generalization across FCD subtypes.

A second line of research explores language-guided segmentation by embedding text semantics into the segmentation pipeline. 
Early methods~\cite{Tomar2022TGANet, Li2023LViT} simply tokenized text and merged embeddings with image features via attention, 
but struggled to capture high-level semantics. 
More recent approaches~\cite{Lee2023TextGuided, Zhong2023Ariadne} employ deep pretrained text encoders (e.g., CXR-BERT~\cite{CXR-BERT}) fused with ConvNeXt-Tiny~\cite{Liu2022ConvNeXt} image features. 
In~\cite{Li2024LGMSeg}, the authors introduce two novel modules: the Target-sensitive Semantic Distance Module (TSDM), 
which computes contrastive distances between segmentation masks to focus on disease-related regions, 
and the Language-guided Target Enhancement Module (LTEM), which applies cross-attention to reinforce critical image areas. 
A bi-directional contrastive loss (averaged cross-entropy in both image $\rightarrow$ text and text $\rightarrow$ image directions) yields more fine-grained guidance 
and enables models trained on 25\% of the data to outperform single-modal baselines trained on full datasets. 
However, these improvements come at the cost of higher architectural complexity and a strong reliance on high-quality textual annotations, 
which may limit robustness and generalizability in real-world clinical settings.

To address unpaired multi-modal data, MulModSeg~\cite{Li2025Mulmodseg} conditions text embeddings on imaging modality using frozen CLIP~\cite{Radford2021CLIP}/BioBERT~\cite{Lee2019BioBERT}/Med-CLIP~\cite{Wang2022MedCLIP} encoders 
combined with medical prompts, and alternates training between vision and text branches (3D-UNet\cite{Cicek2016UNet3D} or SwinUNETR~\cite{Hatamizadeh2022SwinUNETR}). This scheme improves generalization 
without requiring paired \ac{ct}/MR scans. Experiments show that varying the \ac{ct}:MR ratio in training data shifts performance, underscoring the importance 
of balanced modality representation. However, effectiveness depends critically on prompt template design and alternating-training convergence.

% Organ-aware multi-scale segmentation models (OMT-SAM) ~\cite{Zhang2025OrganAware} extend text prompting to specify organs or tissues, overcoming MedSAM’s reliance on geometric 
% prompts. Pretrained CLIP encoders produce paired image/text features at multiple ViT layers; a cross-attention fusion yields rich prompt embeddings. 
% Since small targets are underrepresented by BCE loss alone, combining Dice loss and BCE loss better captures fine structures. OMT-SAM reports 
% improvements in DSC, NSD, and HD95 for organ segmentation, but requires sophisticated text prompt engineering and still depends on ViT performance on 
% small lesions.

Finally, weakly supervised methods such as SimTxtSeg~\cite{Xie2024SimTxtSeg} leverage simple text cues 
(e.g., ``lesion in the left hemisphere'') to guide segmentation without requiring pixel-level annotations. 
SimTxtSeg was evaluated using standard segmentation metrics, including Dice similarity coefficient (DSC), 
Intersection over Union (IoU), Positive Predictive Value (PPV), Normalized Surface Dice (NSD), and the 
95\% Hausdorff Distance (HD95), alongside cross-entropy loss. These results demonstrate that even coarse 
textual hints can substantially improve mask quality over vision-only baselines. However, such approaches 
typically rely on consistent, high-quality text labels and have not yet been validated on rare pathologies 
such as \ac{fcd}.

Building on these insights, we draw inspiration from Ariadne’s Thread~\cite{Zhong2023Ariadne}, which employs 
lightweight text prompts and a GuideDecoder to segment infected regions in chest X-rays. Remarkably, this method 
achieves over a 6\% Dice improvement compared to unimodal baselines while using only 10\% of the training set, 
highlighting the potential of multimodal prompting in low-data regimes. For our FCD task, we adopt the \ac{meld} 
Graph model as a pretrained backbone, since it was trained on the largest dataset among the methods surveyed, 
providing a robust basis for feature extraction and downstream adaptation.

To the best of our knowledge, no prior study has applied text-guided segmentation methods to the \ac{fcd} detection 
task, underscoring the novelty of our approach. In summary, while graph-based GNNs excel at modeling cortical 
geometry and text-guided methods enrich segmentation with semantic context, no existing approach simultaneously 
addresses surface-space lesion detection, integration of fine-grained clinical narratives, robustness to scanner-induced 
domain shifts, and zero-shot generalization to unseen FCD subtypes. In this work, we aim to demonstrate that incorporating 
textual information is also beneficial for the \ac{fcd} detection task. To this end, we integrate different types of text 
descriptions derived from anatomical atlases with vision-based features into a multi-resolution GNN framework, using 
cross-attention fusion and contrastive alignment to enhance robustness and detection accuracy across heterogeneous datasets.


\end{document}

