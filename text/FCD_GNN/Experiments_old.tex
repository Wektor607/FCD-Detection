\documentclass[FCD_GNN.tex]{subfiles}

\begin{document}
\chapter{Experiments}
\label{chapter:Experiments}

In this chapter, we present a comprehensive experimental evaluation of our proposed text-guided \ac{fcd} segmentation framework. We investigate three key factors: (i) how many \ac{meld} feature \emph{stages} are fed into the GNN block; (ii) how many hidden layers from the text encoder are injected into the GuideDecoder; and (iii) the effect of including textual descriptions with varying levels of detail.

This chapter consists of several components. 
First, we define the loss function and evaluation metrics used in all experiments. Then, we conduct systematic ablation studies to understand the contribution of each architectural component. Finally, we analyze the impact of various design choices on model performance both on the main training cohort and on an independent validation dataset.

% We freeze the \ac{meld} backbone. In all variants, we \emph{train} the same geometry-based upsampling path
% (HexUnpool + SpiralConv) and the segmentation head. The variants differ only in whether and how
% the GuideDecoder is used before upsampling:

% \begin{enumerate}
%   \item \textbf{Unpool+Spiral (no text).} No GuideDecoder is used; features are fed directly into the
%         upsampling path (HexUnpool + SpiralConv) and the segmentation head; no textual input.
%   \item \textbf{GuideDecoder (self-attention only).} A GuideDecoder layer is inserted before upsampling,
%         but the text branch is disabled, so only self-attention operates; the same Unpool+Spiral path and
%         segmentation head are trained.
%   \item \textbf{GuideDecoder + Text (full).} The full GuideDecoder is used (self-attention plus
%         cross-attention to the text encoder); textual descriptions are provided, while the same
%         Unpool+Spiral path and segmentation head are trained.
% \end{enumerate}

\section{Loss function}
Following best practices for medical image segmentation and to ensure a fair comparison with the \ac{meld} model, we employed a composite loss. 
The loss function is defined as
\begin{equation}
L = L_{ce} + L_{dice} + L_{dist} + L_{class} + \sum_{i \in I_{ds}} w^i_{ds} \cdot (L^i_{ce} + L^i_{dice} + L^i_{dist})\,
\end{equation}
\par\noindent
The individual components are detailed below. 
% Compared to the \ac{meld} formulation, we replaced the cross-entropy term with Focal Loss in order to mitigate class imbalance between lesional and non-lesional vertices.

\subsection{Cross-Entropy Loss}
For the segmentation task, we employ the binary cross-entropy loss, which is commonly used in medical image segmentation. Here, $y_i$ denotes the ground-truth label, $\hat{y}_i$ the predicted probability, and $n$ the number of vertices:
\[
L_{ce} = - \frac{1}{n} \sum_{i=1}^{n} 
\left[ y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i) \right].
\]

% \subsection{Focal Loss}
% The Focal Loss $L_{foc}$ introduces a modulating factor $(1-\hat{y}_i)^\gamma$ to down-weight easy examples and focus training on harder or misclassified samples. Formally, for ground-truth label $y_i \in \{0,1\}$ and predicted probability $\hat{y}_i$, the focal loss is:
% \[
% L_{foc} = - \frac{1}{n} \sum_{i=1}^{n} \alpha \, (1-\hat{y}_i)^\gamma \, y_i \log(\hat{y}_i) + (1-\alpha) \, \hat{y}_i^\gamma \, (1-y_i) \log(1-\hat{y}_i),
% \]

\subsection{Dice Loss}
The Dice Loss $L_{dice}$ directly optimizes for the overlap between predicted and ground-truth lesion masks. This loss is less sensitive to class imbalance and encourages the network to predict coherent lesion regions. It is defined as
\[
L_{dice} = 1 - \frac{2 \sum_{i=1}^n y_i \hat{y}_i}{\sum_{i=1}^n y_i^2 + \sum_{i=1}^n \hat{y}_i^2 + \epsilon}
\]

It is important to note that we considered two different implementations of this loss. 
In the \href{https://docs.monai.io/en/stable/losses.html}{MONAI library}, the Dice score is calculated for each sample and then averaged over the batch (macro-average), 
whereas in the MELD implementation the aggregation is performed immediately over the entire batch (micro-average). 
Under strong class imbalance between background and lesion voxels, the micro-averaged version shifts the loss contribution towards the background, 
which reduces the gradient signal for rare lesions and leads to training instability. 
For this reason, we used the MONAI implementation, which provided higher metric values and enabled the model to detect more \ac{fcd}s.

\subsection{Distance Loss}
To provide the network with additional contextual information and reduce false positives, we include a distance regression loss $L_{dist}$. 
The model is trained to predict the normalized geodesic distance \( d_i \) from each vertex to the lesion boundary. 
Here, \( \hat{y}_{i,0} \) denotes the model’s output corresponding to the \emph{non-lesional} class for vertex \( i \). 
We employ a mean absolute error loss weighted by \( (d_i + 1)^{-1} \), such that errors near the lesion boundary are penalized more strongly than those in distant non-lesional regions:

\[
L_{dist} = \frac{1}{n} \sum_{i=1}^{n} \frac{|d_i - \hat{y}_{i,0}|}{d_i + 1}.
\]

\subsection{Classification Loss}
And in the last case, we add a weakly-supervised classification head to mitigate the uncertainty between lesion masks and actual lesions. Each subject is labeled as positive if any vertex belongs to a lesion. The classification head aggregates features across the deepest level (level 1) and predicts a subject-level label $\hat{c}$. The classification loss is then computed as binary cross-entropy:
\[
L_{class} = - \sum_{i=1}^{n} c_i \log(\hat{c}_i) + (1-c_i)\log(1-\hat{c}_i).
\]

\subsection{Deep Supervision}
To encourage gradient flow and stabilize training, we adopt deep supervision at intermediate decoder levels $I_{ds} = \{6,5,4,3,2,1\}$. 
At each level $i$, auxiliary predictions are generated and the same combination of cross-entropy, dice, and distance losses is applied. These auxiliary losses are weighted by $w^i_{ds}$ and added to the total objective:
\[
\sum_{i \in I_{ds}} w^i_{ds}(L^i_{ce} + L^i_{dice} + L^i_{dist}).
\]
% This strategy ensures that meaningful supervision is propagated to early layers of the network, improving convergence and overall segmentation accuracy.

\bigskip
% In summary, our loss function combines focal and dice terms for accurate and robust segmentation, a distance regression loss for boundary-awareness, a weakly-supervised classification loss for subject-level consistency, and deep supervision for effective optimization across the entire U-Net.

\section{Metrics}
For evaluating the performance of the model we used several commonly applied metrics in medical image 
analysis: Dice score, \ac{ppv} and \ac{iou}.  
Below we briefly describe each of them.

\subsection{Dice score} 
The Dice similarity coefficient (DSC) is defined as the harmonic mean between precision and recall. 
For two sets of predicted positives $P$ and ground truth positives $G$, it is given by
\[
\mathrm{Dice}(P,G) = \frac{2 \, |P \cap G|}{|P| + |G|} = \frac{2TP}{2TP + FP + FN},
\]
where $TP$, $FP$ and $FN$ denote true positives, false positives and false negatives, respectively.

\subsection{Positive Predictive Value} 
Also known as precision, \ac{ppv} measures the proportion of correctly identified positive samples among all predicted positives:
\[
\mathrm{PPV} = \frac{TP}{TP + FP}.
\]

\subsection{Intersection over Union} 
The \ac{iou} (also called the Jaccard index) quantifies the overlap between predicted and ground truth labels:
\[
\mathrm{IoU}(P,G) = \frac{|P \cap G|}{|P \cup G|} = \frac{TP}{TP + FP + FN}.
\]

% \subsection{Accuracy} 
% Accuracy measures the proportion of correctly classified samples among all samples:
% \[
% \mathrm{ACC} = \frac{TP + TN}{TP + TN + FP + FN},
% \]
% where $TN$ denotes the number of true negatives.

\section{Implementation Details}
The proporsed framework was trained with the following hyperparameters. 
The training batch size was fixed at 8 and the validation batch size at 4. 
The initial learning rate was set to $3 \times 10^{-4}$ and optimized using 
the OneCycleLR scheduler (maximum learning rate $3 \times 10^{-3}$). 
The schedule included a warm-up phase during the first 10\% of training steps, 
followed by cosine annealing. 
Training was performed for up to 100~epochs (minimum 20~epochs), 
with early stopping applied if the validation performance did not improve for 20~epochs.  
For evaluation, we trained an ensemble of five independently initialized models (seeds 42–46). 
At test time, we averaged their per-vertex predictions to form the final output.  

The model integrated surface-based graph features and contextual text embeddings. 
Feature channel dimensions across the encoder stages were \\
$[32,32,64,64,128,128,256]$, 
with corresponding text sequence lengths \\
$[128,64,64,32,32,16,16]$, 
and a maximum text length of 256~tokens. 
Deep supervision was employed with levels 
$I_{ds} = [6,5,4,3,2,1]$ 
and associated weights \\
$w_{ds} = [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.0150765]$. 
The text encoder was initialized from RadBERT, 
with a projection dimension of 768.  

To address class imbalances, non-lesional hemispheres were undersampled, 
ensuring that approximately one third of training examples contained a lesion. 
Data augmentation strategies included random flipping ($p=0.5$), 
Gaussian blur ($p=0.2$), spinning and warping ($p=0.2$ each), 
brightness, contrast, gamma correction and Gaussian noise ($p=0.15$ each), 
and low-resolution simulation ($p=0.25$).

\subsection{Hardware} 
All experiments were performed on the \emph{Bender} high-performance computing cluster at the University of Bonn, 
equipped with NVIDIA A100 GPUs (80~GiB each) and AMD EPYC CPUs. 
A detailed description of the system can be found in the official documentation\footnote{\url{https://www.hpc.uni-bonn.de/en/systems/bender}}. 
Our implementation used PyTorch~2.1.0+cu121, TorchVision~0.16.0+cu121, TorchAudio~2.1.0+cu121, Torch Geometric~2.5.3, Torch Scatter~2.1.2, 
TorchMetrics~0.11.4, and Python~3.9.18.  

\section{Basic Experiments}
Before tuning various hyperparameters such as the number of text connections in the GuideDecoder or the number of unfrozen layers in the LLM, it is essential to first determine the best-performing base model for further experiments. This step saves time and resources by first identifying the most promising model.
  

We freeze the \ac{meld} backbone and unfreeze the last three layers of RadBERT.
In all variants, we \emph{train} the same geometry-based upsampling path 
(\texttt{HexUnpool} + \texttt{SpiralConv}) and the segmentation head. 
The experiments differ only in whether and how the \texttt{GuideDecoder} is employed before upsampling, 
and in whether textual features are incorporated. 
Furthermore, the pretrained \texttt{Exp1} model is used as the initialization decoder for all variants except the MELD model.


We consider the following configurations:

\begin{itemize}
\item \textbf{MELD.} Serves as the baseline model.

    \item \textbf{Exp1 (Unpool + Spiral, no text).} 
    The \texttt{GuideDecoder} is omitted; features are passed directly into the geometry-based upsampling path 
    (\texttt{HexUnpool} + \texttt{SpiralConv}) and the segmentation head. 
    No textual input is used.

    \item \textbf{Exp2 (GuideDecoder, self-attention only).} 
    A \texttt{GuideDecoder} layer is inserted before upsampling, but the text branch is disabled, 
    so only self-attention is applied. 
    The same upsampling path and segmentation head are trained.

    \item \textbf{Exp3\_full (GuideDecoder + Text).} 
    The full \texttt{GuideDecoder} (self-attention and cross-attention to the text encoder) is employed. 
    Textual descriptions are provided using complete Atlas annotations.

    \item \textbf{Exp3\_hemi (GuideDecoder + Text).} 
    Same as \textbf{Exp3\_full}, but Atlas descriptions are reduced to hemisphere names.

    \item \textbf{Exp3\_lobe (GuideDecoder + Text).} 
    Same as \textbf{Exp3\_full}, but Atlas descriptions are reduced to lobe names.

    \item \textbf{Exp3\_hemi+lobe (GuideDecoder + Text).} 
    Same as \textbf{Exp3\_full}, but Atlas descriptions include both hemisphere and lobe names.

    \item \textbf{Exp3\_mixed (GuideDecoder + Text).} 
    Same as \textbf{Exp3\_full}, but Atlas descriptions are randomly sampled from one of 
    \{\emph{hemisphere only}, \emph{lobe only}, \emph{hemisphere + lobe}, \emph{full text}, \emph{no text}\}.
% \item \textbf{Test2 (GuideDecoder + Text).} Same as Exp3\_hemi+lobe, but the decoder is initialized from Exp1 and unfrozen from the first epoch.
% \item \textbf{Test3 (GuideDecoder + Text).} Same as Exp3\_hemi+lobe, but the decoder is initialized from Exp1 and unfrozen only after 10 epochs.
\end{itemize}

% The last two variants were introduced to investigate the impact of using a pre-trained decoder on overall performance. They were applied to the \texttt{Exp3\_hemi+lobe} model, since later analysis will demonstrate that this variant provides the most consistent balance across metrics.

% It is important to note that all experiments in this section were conducted \textbf{without} lesion–mask augmentation, flipping, warping, or spinning. Since some augmentations were found to substantially degrade performance, results with augmentation will be presented in the following chapters.
  
In both tables, the best results are highlighted in green, the second-best in blue, and the third-best in orange.

\paragraph{Main cohort}

% Среди всех результатов модель Exp3: hemi смогла найти больше всего очагов \~ 78\% от общего числа, что на примерно на 12\% лучше, чем у базовой модели MELD. Более того, во всех экспериментах, где применялось текстовое описание,
% модель находила больше очагов, чем MELD, что говорит о полезности текстовых описаний.

% Из полученных результатов видно, что среди всех текстовых моделей хуже всего себя показала модель с полным текстовым описание, что связано с избыточностью информации в полном описании. Тем не менее, модель Exp3: hemi+lobe показывает, что добавление информации о 
% названии полушария позволяет улучшить качество сегментации судя по метрикам PPV, хоть и заметен проигрыш по количеству найденных очагов. Важно отметить, что авторы MELD считали, что очаг найден, если предсказание попадало в радиус 20 mm от центра масс очага, что мы собственно также и делаем.

% Также интересно, что изспользование смешанного типа текстовых предсказаний позволяет добиться лучшего качества по Dice, IoU и лучшего PPV_ckusters среди Exp3. 

Among all models, \textbf{Exp3: hemi} detected the largest number of lesions
($202 / 259$, $\sim$78\% of the total), which is roughly 12\% higher than the baseline MELD model ($170 / 259$).
Moreover, in all experiments involving textual descriptions, the models
identified more lesions than MELD, indicating the usefulness of text-based conditioning.

Among the text-guided variants, the \textbf{full description} model performed the worst,
likely due to the redundancy and noise in the long textual input.
In contrast, \textbf{Exp3: hemi+lobe} shows that adding lobe information
can improve segmentation quality according to PPV metrics, although with a
slight decrease in sensitivity. Notably, following the MELD evaluation protocol,
a lesion was considered ``detected'' if the predicted region fell within a 20~mm
radius of the lesion's center of mass.

Using \textbf{mixed textual descriptions} led to the highest
overall overlap quality (Dice and IoU) and the best PPV among all Exp3 variants,
suggesting that concise, contextually varied text prompts offer a good balance
between detection rate and precision.

\begin{table}[ht]
\centering
\caption{Median performance on the main cohort.}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Dice} &
\makecell{\textbf{PPV}\\\textbf{pixels}} &
\makecell{\textbf{PPV}\\\textbf{clusters}} &
\textbf{IoU} & \textbf{Sensitivity} \\
\midrule
MELD & \makecell{0.230\\(0.107--0.320)} & \makecell{0.166\\(0.086--0.220)} & \colorbox{green!20}{0.704} & \makecell{0.130\\(0.057--0.190)} & 170 / 259 \\
Exp1 & \makecell{0.240\\(0.129--0.324)} & \makecell{0.217\\(0.151--0.319)} & \colorbox{orange!20}{0.532} & \makecell{0.136\\(0.069--0.193)} & 180 / 259 \\
Exp2 & \makecell{0.208\\(0.088--0.325)} & \makecell{\colorbox{blue!20}{0.230}\\(0.110--0.302)} & \colorbox{blue!20}{0.603} & \makecell{0.116\\(0.046--0.194)} & 172 / 259 \\
Exp3: hemi & \makecell{\colorbox{blue!20}{0.248}\\(0.188--0.327)} & \makecell{0.209\\(0.150--0.325)} & 0.439 & \makecell{\colorbox{orange!20}{0.141}\\(0.104--0.196)} & \colorbox{green!20}{202 / 259} \\
Exp3: lobe & \makecell{\colorbox{blue!20}{0.248}\\(0.117--0.327)} & \makecell{0.219\\(0.140--0.318)} & 0.516 & \makecell{\colorbox{blue!20}{0.142}\\(0.062--0.195)} & 180 / 259 \\
Exp3: hemi+lobe & \makecell{\colorbox{orange!20}{0.246}\\(0.145--0.316)} & \makecell{\colorbox{green!20}{0.246}\\(0.173--0.339)} & 0.464 & \makecell{0.140\\(0.078--0.188)} & \colorbox{orange!20}{187 / 259} \\
Exp3: full\_desc & \makecell{0.241\\(0.169--0.311)} & \makecell{0.190\\(0.151--0.285)} & 0.371 & \makecell{0.137\\(0.092--0.184)} & \colorbox{blue!20}{188 / 259} \\
Exp3: mixed\_desc & \makecell{\colorbox{green!20}{0.251}\\(0.126--0.320)} & \makecell{\colorbox{orange!20}{0.225}\\(0.155--0.345)} & 0.521 & \makecell{\colorbox{green!20}{0.144}\\(0.067--0.190)} & 178 / 259 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Independent cohort}

% On the Bonn dataset, результаты не так похожи как на основной когорте.

% По качеству лучше всего себя показала модель Exp2 по метрика Dice и Iou, однако стоит обратить внимание на доверительный интервал. Разброс значений по сравнению с другими моделями очень большой, что говорит о нестабильности модели.

% Среди моделей с текстовыми описаниями, \texttt{Exp3: hemi} нашла больше всего очагов и остальные модели с текстом нашли либо столько же, либо больше очагов чем MELD, что еще раз подтверждает гипотезу, что текстовые описания полезны для поиска очагов.

% Среди всех моделей, \texttt{Exp3: hemi+lobe} показывает хороший компромисс между качеством и количеством среди моделей с текстом. Снова видно по PPV метрикам, что добавление lobe к hemi дает прирост в качестве. 

\noindent
Performance trends are less aligned with the main cohort, with visibly wider confidence intervals.
\textbf{Exp2} attains the best median Dice and IoU but exhibits unusually broad CIs
(e.g., intervals extending close to zero), suggesting instability and possible overfitting.

Among text-guided models, \textbf{Exp3: hemi} again delivers the highest sensitivity (65/82),
whereas \textbf{Exp3: hemi+lobe} provides a reasonable balance between sensitivity and precision,
echoing the benefit of concise, structured priors (hemisphere and lobe) over verbose descriptions.

\begin{table}[ht]
\centering
\caption{Median performance on the cohort (no color highlighting).}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Dice} &
\makecell{\textbf{PPV}\\\textbf{pixels}} &
\makecell{\textbf{PPV}\\\textbf{clusters}} &
\textbf{IoU} & \textbf{Sensitivity} \\
\midrule
MELD & \makecell{0.358\\(0.209--0.465)} & \makecell{0.261\\(0.142--0.428)} & \colorbox{green!20}{0.784} & \makecell{0.218\\(0.117--0.303)} & 57 / 82 \\
Exp1 & \makecell{0.371\\(0.289--0.520)} & \makecell{\colorbox{blue!20}{0.438}\\(0.246--0.660)} & \colorbox{blue!20}{0.680} & \makecell{0.227\\(0.169--0.351)} & \colorbox{orange!20}{60 / 82} \\
Exp2 & \makecell{\colorbox{green!20}{0.394}\\\colorbox{red!20}{(0.002--0.544)}} & \makecell{0.411\\\colorbox{red!20}{(0.006--0.636)}} & \colorbox{orange!20}{0.679} & \makecell{\colorbox{green!20}{0.246}\\\colorbox{red!20}{(0.001--0.373)}} & 53 / 82 \\
Exp3: hemi & \makecell{\colorbox{blue!20}{0.390}\\(0.235--0.490)} & \makecell{0.318\\(0.188--0.546)} & 0.433 & \makecell{\colorbox{blue!20}{0.242}\\(0.133--0.324)} & \colorbox{green!20}{65 / 82} \\
Exp3: lobe & \makecell{0.337\\(0.220--0.504)} & \makecell{0.356\\(0.171--0.639)} & 0.610 & \makecell{0.203\\(0.124--0.337)} & 57 / 82 \\
Exp3: hemi + lobe& \makecell{0.341\\(0.232--0.514)} & \makecell{\colorbox{green!20}{0.444}\\(0.296--0.637)} & 0.536 & \makecell{0.206\\(0.131--0.346)} & \colorbox{orange!20}{60 / 82} \\
Exp3: full\_desc & \makecell{0.337\\(0.207--0.519)} & \makecell{0.377\\(0.221--0.596)} & 0.450 & \makecell{0.203\\(0.115--0.350)} & \colorbox{blue!20}{62 / 82} \\
Exp3: mixed & \makecell{\colorbox{orange!20}{0.385}\\(0.236--0.499)} & \makecell{\colorbox{orange!20}{0.434}\\(0.209--0.722)} & 0.594 & \makecell{\colorbox{orange!20}{0.238}\\(0.134--0.333)} & 57 / 82 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{General conclusions}
\begin{itemize} %[noitemsep, topsep=0pt]
    \item \texttt{Exp3\_hemi} consistently detects the largest number of lesions across datasets, achieving the highest sensitivity. This demonstrates that even concise anatomical text prompts can effectively guide the model toward focal regions.
    
    \item \texttt{Exp3\_hemi+lobe} represents the most balanced and stable configuration, showing robust performance across datasets and metrics. The inclusion of both hemisphere and lobe information appears to improve precision while maintaining high sensitivity.
    
    \item \texttt{Exp3\_full} yielded the weakest results among all text-conditioned models. To confirm that full textual descriptions from the atlas introduce redundant or noisy information, a dedicated analysis was performed in a separate chapter by varying the number of unfrozen layers in the RadBERT encoder.
    
    \item \texttt{Exp3\_mixed} consistently achieves the highest Dice and IoU scores, suggesting that diverse and contextually varied textual prompts enhance the overall segmentation quality and model generalization.
    
    % \item The \texttt{MELD baseline} underperforms across most metrics compared to the proposed text-guided variants, confirming the added value of linguistic conditioning and the adapted decoder architecture.
\end{itemize}

% The full results with confidence intervals are reported in \autoref{tab:main_full} and \autoref{tab:bonn_full}.

% TODO ######################################################
\section{Linking MELD to GNN}
In these experiments we investigated the effect of connecting different numbers of MELD feature stages to the GNN block. The rationale was that higher MELD stages may produce sparse representations, while lower stages provide richer local detail. By progressively adding stages from top to bottom, we aimed to evaluate how multi-stage integration influences model performance. To isolate this effect, the text encoder and GuideDecoder were disabled.
% In our initial experiments, after combining vision and text features, the obtained results were of relatively low quality. 
% This observation suggested that the output representations produced by MELD at higher stages are rather sparse, which may limit the predictive capacity of the model. 
% To address this, we hypothesized that aggregating information from multiple MELD stages could improve the expressivity of the GNN block and lead to more accurate predictions. 
% Therefore, we conducted a series of experiments in which different numbers of MELD feature stages were connected to the GNN block, ranging from high-level features to lower-level ones, in order to evaluate the impact of stage-wise integration on model performance.
% And in order to show exactly the influence of the GNN block, we disabled the text model and GuideDecoder.

% A short clarification regarding the experiment names: 
% \texttt{Exp1 1 layer} means that only the highest-level MELD features were connected to the GNN block, while 
% \texttt{Exp1 0 layer} corresponds to skipping the GNN block entirely and feeding MELD features directly. 
% As the number of layers increases, additional MELD stages are progressively included from top to bottom, 
% so that lower-level representations are added step by step to the higher-level ones.
\begin{table}[ht]
\centering
\footnotesize
\caption{Different number of MELD stages connected to the GNN block (values in parentheses indicate 95\% confidence intervals). Main cohort}
\begin{tabular}{lccccc}
\hline
\textbf{Experiment} & \textbf{Dice} & \textbf{PPV pixels} & \textbf{PPV clusters} & \textbf{IoU} & \textbf{Number of FCD found} \\
\hline
Exp1: 0 layers  & \makecell{\colorbox{blue!20}{0.240} \\ (0.129--0.324)} & \makecell{0.217 \\ (0.151--0.319)} & 0.532 & \makecell{\colorbox{blue!20}{0.136} \\ (0.069--0.193)} & 180 / 259 \\
Exp1: 1 layer   & \makecell{\colorbox{orange!20}{0.235} \\ (0.096--0.313)} & \makecell{\colorbox{green!20}{0.235} \\ (0.148--0.362)} & \colorbox{green!20}{0.603} & \makecell{\colorbox{orange!20}{0.133} \\ (0.051--0.186)} & 174 / 259 \\
Exp1: 2 layers  & \makecell{0.234 \\ (0.126--0.334)} & \makecell{\colorbox{orange!20}{0.219} \\ (0.136--0.371)} & \colorbox{blue!20}{0.573} & \makecell{0.132 \\ (0.067--0.200)} & 177 / 259 \\
Exp1: 3 layers  & \makecell{\colorbox{green!20}{0.249} \\ (0.114--0.332)} & \makecell{\colorbox{blue!20}{0.224} \\ (0.142--0.341)} & 0.479 & \makecell{\colorbox{green!20}{0.142} \\ (0.060--0.199)} & 181 / 259 \\
Exp1: 4 layers  & \makecell{0.231 \\ (0.097--0.304)} & \makecell{0.180 \\ (0.128--0.283)} & \colorbox{orange!20}{0.540} & \makecell{0.130 \\ (0.051--0.179)} & 177 / 259 \\
Exp1: 5 layers  & \makecell{0.223 \\ (0.081--0.312)} & \makecell{0.172 \\ (0.090--0.261)} & 0.533 & \makecell{0.125 \\ (0.042--0.185)} & 171 / 259 \\
Exp1: 6 layers  & \makecell{0.208 \\ (0.121--0.312)} & \makecell{\colorbox{orange!20}{0.219} \\ (0.147--0.336)} & 0.539 & \makecell{0.116 \\ (0.065--0.185)} & 179 / 259 \\
Exp1: 7 layers  & \makecell{0.206 \\ (0.119--0.293)} & \makecell{\colorbox{orange!20}{0.219} \\ (0.135--0.341)} & 0.436 & \makecell{0.115 \\ (0.063--0.172)} & 188 / 259 \\
\hline
\end{tabular}
\end{table}


% Among all models, \texttt{Exp1} with 6 layers performed best overall: it detected the largest number of lesions and achieved the highest Dice and IoU scores, but this came at the cost of reduced precision due to a large number of false positives.  

% In contrast, \texttt{Exp1} with 3 layers showed a more balanced behavior: its PPV metrics ranked second-best, indicating fewer false positives, but it missed more lesions compared to the 6-layer variant.  

\begin{table}[ht]
\centering
\footnotesize
\caption{Different number of MELD stages connected to the GNN block (values in parentheses indicate 95\% confidence intervals). Independent cohort (Bonn Dataset)}
\begin{tabular}{lccccc}
\hline
\textbf{Experiment} & \textbf{Dice} & \textbf{PPV pixels} & \textbf{PPV clusters} & \textbf{IoU} & \textbf{Number of FCD found} \\
\hline
Exp1: 0 layers &
\makecell{0.371 \\ (0.289--0.520)} &
\makecell{0.438 \\ (0.246--0.660)} &
\makecell{\colorbox{orange!20}{0.680}} &
\makecell{0.227 \\ (0.169--0.351)} &
\makecell{60 / 82} \\
%
Exp1: 1 layer &
\makecell{0.342 \\ (0.083--0.515)} &
\makecell{\colorbox{blue!20}{0.528} \\ (0.242--0.757)} &
\makecell{\colorbox{green!20}{0.819}} &
\makecell{0.206 \\ (0.043--0.347)} &
\makecell{54 / 82} \\
%
Exp1: 2 layers &
\makecell{0.383 \\ (0.216--0.506)} &
\makecell{0.432 \\ (0.243--0.663)} &
\makecell{\colorbox{blue!20}{0.729}} &
\makecell{0.237 \\ (0.123--0.339)} &
\makecell{58 / 82} \\
%
Exp1: 3 layers &
\makecell{0.390 \\ (0.302--0.517)} &
\makecell{0.503 \\ (0.309--0.732)} &
\makecell{0.619} &
\makecell{0.242 \\ (0.178--0.349)} &
\makecell{\colorbox{blue!20}{62 / 82}} \\
%
Exp1: 4 layers &
\makecell{\colorbox{green!20}{0.458} \\ (0.279--0.600)} &
\makecell{0.491 \\ (0.186--0.582)} &
\makecell{0.653} &
\makecell{\colorbox{green!20}{0.297} \\ (0.162--0.428)} &
\makecell{60 / 82} \\
%
Exp1: 5 layers &
\makecell{\colorbox{blue!20}{0.432} \\ (0.303--0.549)} &
\makecell{0.410 \\ (0.249--0.606)} &
\makecell{0.555} &
\makecell{\colorbox{blue!20}{0.275} \\ (0.179--0.378)} &
\makecell{\colorbox{green!20}{63 / 82}} \\
%
Exp1: 6 layers &
\makecell{0.382 \\ (0.292--0.544)} &
\makecell{\colorbox{green!20}{0.584} \\ (0.342--0.687)} &
\makecell{0.644} &
\makecell{0.236 \\ (0.171--0.373)} &
\makecell{60 / 82} \\
%
Exp1: 7 layers &
\makecell{0.402 \\ (0.284--0.493)} &
\makecell{0.375 \\ (0.232--0.634)} &
\makecell{0.451} &
\makecell{\colorbox{orange!20}{0.252} \\ (0.165--0.327)} &
\makecell{\colorbox{orange!20}{61 / 82}} \\
\hline
\end{tabular}
\end{table}

% Across both cohorts, a clear trade-off emerges. Models with a larger number of connected MELD stages (e.g., with 6 layers) achieved the highest Dice and IoU scores and detected the most lesions, but at the expense of lower precision, as indicated by reduced PPV values. In contrast, configurations with fewer connected stages (e.g., 5 layers) provided more balanced results, yielding higher PPV and thus fewer false positives, but missing more lesions overall.

% Given that the primary goal of this work is to maximize lesion detection (i.e., higher sensitivity), we adopt the 6-layer configuration for subsequent experiments. Nevertheless, the 5-layer setting appears preferable in scenarios where precision and false-positive control are prioritized.
% On the independent Bonn dataset, the situation is similar. \texttt{Exp1} with 6 layers again achieved the highest Dice and IoU scores and detected the largest number of lesions, but its PPV dropped significantly, confirming the increase in false positives. The 3-layer model again provided a reasonable compromise between precision and sensitivity, although with lower Dice and IoU.  

% The results show that connecting a larger number of MELD stages to the GNN block increases lesion sensitivity and overlap quality (Dice, IoU), but simultaneously reduces precision, as reflected by lower PPV values. 
% A lower PPV means that, although multi-stage integration improves the detection of true lesions, it also increases the number of false positive clusters and thereby reduces the reliability of the predictions.
% Because the objective of this study is to maximize the number of detected lesions, the 6-layer \texttt{Exp1} model is used in subsequent experiments.



% \section{Text Guidance in Decoder}
% % In this chapter, we investigate the influence of textual descriptions on the segmentation quality.
% % To this end, we conducted a series of experiments in which we unfroze different numbers of RadBERT layers, starting from 0 (all layers frozen) up to 12 (all layers unfrozen) in steps of 3.
% % In principle, one could also perform unfreezing with a step size of 1, but besides being time-consuming, we believe it would not yield substantial improvements in performance. 
% % An exception was made for the first 3 layers, which we evaluated separately. 

% To assess the influence of textual descriptions on segmentation quality, we conducted experiments with different numbers of unfrozen RadBERT layers. We compared settings from a fully frozen text encoder (0 layers) up to complete fine-tuning (12 layers), with intermediate configurations in steps of three layers. To better assess the effect of unfreezing a small number of layers, we additionally evaluated a configuration with the first three layers unfrozen.
% \begin{table}[ht]
% \centering
% \footnotesize
% \caption{Different number of unfreezed layers in RadBERT}
% \begin{tabular}{lccccc}
% \hline
% \textbf{Experiment} & \textbf{Dice} & \textbf{PPV pixels} & \textbf{PPV clusters} & \textbf{IoU} & \textbf{Number of FCD found} \\
% \hline
% 0 layers & \textbf{0.286} & 0.214 & 0.542 & \textbf{0.167} & \textbf{172 / 259} \\
% 3 layers & 0.281 & 0.244 & 0.630 & 0.163 & 164 / 259 \\
% 6 layers & 0.256 & 0.223 & 0.587 & 0.147 & 166 / 259 \\
% 9 layers & 0.245 & \textbf{0.263} & \textbf{0.634} & 0.139 & 164 / 259 \\
% 12 layers & 0.286 & 0.202 & 0.554 & 0.167 & 168 / 259 \\
% \hline
% \end{tabular}
% \end{table}

% % 0 layers
% % Dice : 0.286 (95% CI 0.168-0.345)
% % PPV_pixels  : 0.214 (95% CI 0.163-0.303)
% % PPV_clusters  :  0.5417789757412399
% % IoU  : 0.167 (95% CI 0.092-0.209)
% % \section{Effect of Textual Descriptions}
% % 3 layers
% % 0.281 (95% CI 0.171-0.359)
% % PPV_pixels  : 0.244 (95% CI 0.134-0.306)
% % PPV_clusters  :  0.6295081967213115
% % IoU  : 0.163 (95% CI 0.094-0.219)
% % 6 layers
% % 0.256 (95% CI 0.159-0.327)
% % PPV_pixels  : 0.223 (95% CI 0.171-0.325)
% % PPV_clusters  :  0.5865921787709497
% % IoU  : 0.147 (95% CI 0.087-0.196)
% % 9 layers
% % 0.245 (95% CI 0.130-0.357)
% % PPV_pixels  : 0.263 (95% CI 0.133-0.364)
% % PPV_clusters  :  0.6335616438356164
% % IoU  : 0.139 (95% CI 0.070-0.217)
% % 12
% % 0.286 (95% CI 0.156-0.334)
% % PPV_pixels  : 0.202 (95% CI 0.156-0.299)
% % PPV_clusters  :  0.5538057742782152
% % IoU  : 0.167 (95% CI 0.084-0.200)
% \begin{table}[ht]
% \centering
% \footnotesize
% \caption{Different number of unfreezed layers in RadBERT. Independent cohort (Bonn Dataset)}
% \begin{tabular}{lccccc}
% \hline
% \textbf{Experiment} & \textbf{Dice} & \textbf{PPV pixels} & \textbf{PPV clusters} & \textbf{IoU} & \textbf{Number of FCD found} \\
% \hline
% 0 layers & \textbf{0.483} & 0.427 & 0.582 & \textbf{0.319} & \textbf{55 / 82} \\
% 3 layers & 0.456 & 0.415 & \textbf{0.803} & 0.295 & 52 / 82 \\
% 6 layers & 0.367 & 0.398 & 0.655 & 0.225 & 51 / 82 \\
% 9 layers & 0.411 & \textbf{0.443} & 0.721 & 0.258 & 52 / 82 \\
% 12 layers & 0.444 & 0.371 & 0.663 & 0.286 & 53 / 82 \\
% \hline
% \end{tabular}
% \end{table}

% The results show a consistent trend across both cohorts. With a fully frozen text encoder (0 layers), the model achieved the highest Dice and IoU, as well as the largest number of detected lesions. This suggests that extensive fine-tuning of RadBERT does not improve overall segmentation performance and may even degrade it. 

% At the same time, partially unfrozen models (especially with 3 or 9 layers) yielded higher \textit{PPV\_clusters} values, indicating fewer false positive clusters and thus a more precise localization of lesions. In particular, the improvement between 0 layers and 3 layers amounts to approximately 9\% in the main cohort and about 22\% in the independent cohort. 

% Taken together, these findings reveal a trade-off: freezing RadBERT maximizes sensitivity (number of lesions found), while partial unfreezing improves precision (PPV). Since our primary objective in this study is to detect as many lesions as possible, we adopt the frozen configuration as the default in subsequent experiments. Nevertheless, for applications prioritizing precision, limited fine-tuning of the text encoder may be beneficial.

% % We performed experiments both on the main cohort and on an independent one (Bonn Dataset). 
% % The results show that with a fully frozen text model we achieved the best Dice and IoU, as well as the largest number of detected lesions. 
% % However, according to the PPV metrics, the quality of the predictions was the worst among all settings, indicating that the model produced many false positives. 
% % This difference is particularly visible in the PPV\_clusters metric, where the gap between 0 layers and the best result (3 layers) amounts to approximately 9\% in the main cohort and about 22\% in the independent one. 

% % Thus, the question arises again of what setting should be considered optimal. 
% % Since in this work our primary goal is to detect as many lesions as possible, even at the cost of a slight drop in quality, we decided to use the frozen model for subsequent experiments. 
% % Nevertheless, we do not exclude the possibility that with other datasets or RadBERT hyperparameters, an unfrozen model may perform better.% 0 layers

% % 0 layers
% % Dice: 0.483 (95% CI 0.282-0.574)
% % PPV_pixels  : 0.427 (95% CI 0.237-0.591)
% % PPV_clusters  :  0.5825242718446602
% % IoU  : 0.319 (95% CI 0.164-0.403)
% % 1 layer
% % Dice: 0.439 (95% CI 0.258-0.554)
% % PPV_pixels  : 0.390 (95% CI 0.198-0.656)
% % PPV_clusters  :  0.6829268292682927
% % IoU  : 0.282 (95% CI 0.148-0.384)
% % 3 layers
% % Dice: 0.456 (95% CI 0.007-0.559)
% % PPV_pixels  : 0.415 (95% CI 0.008-0.648)
% % PPV_clusters  :  0.8028169014084507
% % IoU  : 0.295 (95% CI 0.004-0.388)
% % 6 layers
% % Dice: 0.367 (95% CI 0.156-0.523)
% % PPV_pixels  : 0.398 (95% CI 0.114-0.569)
% % PPV_clusters  :  0.6551724137931034
% % IoU  : 0.225 (95% CI 0.091-0.354)
% % 9 layers
% % Dice: 0.411 (95% CI 0.070-0.545)
% % PPV_pixels  : 0.443 (95% CI 0.131-0.619)
% % PPV_clusters  :  0.7215189873417721
% % IoU  : 0.258 (95% CI 0.036-0.375)
% % 12 layers
% % 0.444 (95% CI 0.216-0.546)
% % PPV_pixels  : 0.371 (95% CI 0.177-0.474)
% % PPV_clusters  :  0.6629213483146067
% % IoU  : 0.286 (95% CI 0.123-0.376)

% \section{Text–GuideDecoder Connections}
% In these experiments, we examine how the number of LLM connections to the GuideDecoder affects the final performance.
% We tested configurations ranging from 0 connections (no text guidance) to 6 connections (text features connected to all GuideDecoder layers).
% \begin{table}[ht]
% \centering
% \footnotesize
% \caption{Different number of unfreezed layers in RadBERT}
% \begin{tabular}{lccccc}
% \hline
% \textbf{Experiment} & \textbf{Dice} & \textbf{PPV pixels} & \textbf{PPV clusters} & \textbf{IoU} & \textbf{Number of FCD found} \\
% \hline
% 0 connections  & 0.234 & 0.196 & 0.654 & 0.133 & 162 / 259 \\
% 1 connection   & 0.243 & 0.184 & 0.548 & 0.138 & 169 / 259 \\
% 2 connections  & 0.123 & 0.116 & 0.689 & 0.066 & 151 / 259 \\
% 3 connections  & 0.074 & 0.076 & \textbf{0.761} & 0.038 & 139 / 259 \\
% 4 connections  & \textbf{0.282} & 0.210 & 0.540 & \textbf{0.164} & \textbf{170 / 259} \\
% 5 connections  & 0.234 & 0.223 & 0.614 & 0.133 & 161 / 259 \\
% 6 connections  & 0.245 & \textbf{0.227} & 0.597 & 0.140 & 163 / 259 \\
% \hline
% \end{tabular}
% \end{table}
% % 6 connections
% % Dice: 0.245 (95% CI 0.100-0.364)
% % PPV_pixels  : 0.227 (95% CI 0.152-0.349)
% % PPV_clusters  :  0.5975232198142415
% % IoU  : 0.140 (95% CI 0.052-0.222)
% % 5 connections 
% % 0.234 (95% CI 0.114-0.356)
% % PPV_pixels  : 0.223 (95% CI 0.138-0.378)
% % PPV_clusters  :  0.6141975308641975
% % IoU  : 0.133 (95% CI 0.061-0.216)
% % 4 connections
% % 0.282 (95% CI 0.166-0.340)
% % PPV_pixels  : 0.210 (95% CI 0.135-0.266)
% % PPV_clusters  :  0.5403899721448467
% % IoU  : 0.164 (95% CI 0.091-0.205)
% % 3 connections
% % Dice: 0.074 (95% CI 0.000-0.264)
% % PPV_pixels  : 0.076 (95% CI 0.000-0.230)
% % PPV_clusters  :  0.7616580310880829
% % IoU  : 0.038 (95% CI 0.000-0.152)
% % 2 connections
% % Dice: 0.123 (95% CI 0.035-0.294)
% % PPV_pixels  : 0.116 (95% CI 0.035-0.209)
% % PPV_clusters  :  0.689795918367347
% % IoU  : 0.066 (95% CI 0.018-0.173)
% % 1 connection
% % Dice: 0.243 (95% CI 0.158-0.318)
% % PPV_pixels  : 0.184 (95% CI 0.100-0.229)
% % PPV_clusters  :  0.5482954545454546
% % IoU  : 0.138 (95% CI 0.086-0.189)
% \begin{table}[ht]
% \centering
% \footnotesize
% \caption{Different number of unfreezed layers in RadBERT. Independent cohort (Bonn Dataset)}
% \begin{tabular}{lccccc}
% \hline
% \textbf{Experiment} & \textbf{Dice} & \textbf{PPV pixels} & \textbf{PPV clusters} & \textbf{IoU} & \textbf{Number of FCD found} \\
% \hline
% 0 connections & 0.392 & 0.352 & 0.700 & 0.244 & 49 / 82 \\
% 1 connection & 0.413 & 0.332 & 0.545 & 0.260 & \textbf{55 / 82} \\
% 2 connections & 0.339 & 0.287 & 0.787 & 0.204 & 50 / 82 \\
% 3 connections & 0.319 & 0.296 & \textbf{0.875} & 0.189 & 46 / 82 \\
% 4 connections & \textbf{0.433} & 0.352 & 0.555 & \textbf{0.276}  & \textbf{55 / 82} \\
% 5 connections & 0.406 & 0.397 & 0.682 & 0.255 & 50 / 82 \\
% 6 connections & 0.378 & \textbf{0.422} & 0.760 & 0.233 & 50 / 82 \\
% \hline
% \end{tabular}
% \end{table}

% In both experiments, using 4 connections yielded the best overall performance, achieving the highest Dice score and IoU, while also detecting the largest number of lesions. This suggests that a moderate level of text guidance provides the optimal balance between leveraging textual information and preserving model stability.

% Notably, the difference between using no text guidance (0 connections) and applying it everywhere (6 connections) was relatively small. This implies that excessive text integration may cause overfitting or redundancy, where additional textual connections fail to provide meaningful improvements and can even introduce noise that degrades performance.

% Therefore, we adopted 4 connections for all subsequent experiments.
% % 6 connections
% % Dice: 0.378 (95% CI 0.109-0.539)
% % PPV_pixels  : 0.422 (95% CI 0.122-0.672)
% % PPV_clusters  :  0.7605633802816901
% % IoU  : 0.233 (95% CI 0.057-0.369)
% % 5 connections
% % 0.406 (95% CI 0.041-0.533)
% % PPV_pixels  : 0.397 (95% CI 0.101-0.617)
% % PPV_clusters  :  0.6823529411764706
% % IoU  : 0.255 (95% CI 0.021-0.363)
% % 4 connections
% % 0.433 (95% CI 0.236-0.567)
% % PPV_pixels  : 0.352 (95% CI 0.168-0.498)
% % PPV_clusters  :  0.5555555555555556
% % IoU  : 0.276 (95% CI 0.134-0.396)
% % 3 connections
% % 0.319 (95% CI 0.000-0.542)
% % PPV_pixels  : 0.296 (95% CI 0.000-0.581)
% % PPV_clusters  :  0.875
% % IoU  : 0.189 (95% CI 0.000-0.372)
% % 2 connections
% % Dice: 0.339 (95% CI 0.035-0.545)
% % PPV_pixels  : 0.287 (95% CI 0.068-0.486)
% % PPV_clusters  :  0.7878787878787878
% % IoU  : 0.204 (95% CI 0.018-0.374)
% % 1 connection
% % 0.413 (95% CI 0.245-0.550)
% % PPV_pixels  : 0.332 (95% CI 0.188-0.443)
% % PPV_clusters  :  0.5454545454545454
% % IoU  : 0.260 (95% CI 0.141-0.379)


\end{document}
