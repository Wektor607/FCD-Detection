import os
import torch
import argparse
import subprocess
import random
import csv
import numpy as np
import pandas as pd

import torch.nn as nn
from torchmetrics import Accuracy, Dice
from torchmetrics.classification import BinaryJaccardIndex, Precision

import torch.multiprocessing
from torch.utils.data import DataLoader, Sampler

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger

from sklearn.model_selection import KFold
from transformers import AutoTokenizer

import wandb
import utils.config as config
from utils.data import EpilepDataset
from engine.wrapper import LanGuideMedSegWrapper
from meld_graph.paths import FS_SUBJECTS_PATH
from meld_graph.meld_cohort import MeldCohort

# —Ç–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å
torch.multiprocessing.set_sharing_strategy("file_system")


SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# –ü–æ–ª–Ω–∞—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ TF32 (–¥–∞—ë—Ç –º–µ–Ω—å—à–∏–π –¥—Ä–µ–π—Ñ –Ω–∞ Ampere/Ada)
torch.use_deterministic_algorithms(True, warn_only=True)
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
# (–æ–ø—Ü.) –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
# torch.set_float32_matmul_precision("high")


# –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ DataLoader —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–æ—Ä–∫–µ—Ä–∞–º–∏:
def worker_init_fn(worker_id):
    np.random.seed(SEED + worker_id)
    random.seed(SEED + worker_id)


def get_parser():
    parser = argparse.ArgumentParser(
        description="Language-guide Medical Image Segmentation"
    )
    parser.add_argument(
        "--config", default="./config/training.yaml", type=str, help="config file"
    )
    parser.add_argument("--meld_check", default=False, type=bool, help="config file")

    args = parser.parse_args()
    assert args.config is not None
    cfg = config.load_cfg_from_cfg_file(args.config)
    cfg.meld_check = args.meld_check
    return cfg


def mgh_cleaner():
    cleanup_cmd = [
        "find",
        os.path.join(FS_SUBJECTS_PATH),
        "-type",
        "f",
        "-path",
        "*/xhemi/classifier/*",
        "-delete",
    ]
    subprocess.run(cleanup_cmd, check=True)


class LesionOversampleSampler(Sampler):
    """
    –°—ç–º–ø–ª–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –±–µ—Ä—ë—Ç –í–°–ï healthy-–ø—Ä–∏–º–µ—Ä—ã —Ä–æ–≤–Ω–æ –ø–æ –æ–¥–Ω–æ–º—É —Ä–∞–∑—É,
    –∞ lesion-–ø—Ä–∏–º–µ—Ä—ã ‚Äî —Å replacement, —á—Ç–æ–±—ã –∑–∞–ø–æ–ª–Ω–∏—Ç—å –≤—Å—é —ç–ø–æ—Ö—É.
    """

    def __init__(self, labels, seed=42):
        self.labels = labels
        random.seed(seed)
        # –∏–Ω–¥–µ–∫—Å—ã –∑–¥–æ—Ä–æ–≤—ã—Ö –∏ lesion
        self.hc_idx = [i for i, label in enumerate(labels) if label == 0]
        self.les_idx = [i for i, label in enumerate(labels) if label == 1]
        # —Ö–æ—Ç–∏–º —Ä–æ–≤–Ω–æ len(labels) –≤—ã–±–æ—Ä–æ–∫ –∑–∞ —ç–øo—Ö—É
        self.epoch_size = len(labels)

    def __iter__(self):
        # –Ω–∞—á–∏–Ω–∞–µ–º —Å –≤—Å–µ—Ö hc-–∏–Ω–¥–µ–∫—Å–æ–≤
        idxs = self.hc_idx.copy()
        # —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –¥–æ–∫–∏–Ω—É—Ç—å lesion'–æ–≤
        n_les_to_sample = self.epoch_size - len(idxs)
        # –¥–æ–±–∞–≤–ª—è–µ–º lesion —Å replacement
        idxs += random.choices(self.les_idx, k=n_les_to_sample)
        # –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –≤—Å—é —ç–ø–æ—Ö—É
        random.shuffle(idxs)
        return iter(idxs)

    def __len__(self):
        return self.epoch_size


def iter_labels_from_batch(batch):
    """
    –ü—Ä–∏–≤–µ–¥–∏ –∫ —Å–≤–æ–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ: –≤–µ—Ä–Ω–∏ —Ç–µ–Ω–∑–æ—Ä –º–µ—Ç–æ–∫ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ.
    –î–æ–ª–∂–µ–Ω –±—ã—Ç—å bool/0-1, shape [B, H, N] –∏–ª–∏ [B, N] –∏–ª–∏ [B*N].
    """
    # –ø—Ä–∏–º–µ—Ä ‚Äî –ø–æ–¥—Å—Ç—Ä–æ–π –ø–æ–¥ —Å–≤–æ–π batch
    # –∏–∑ —Ç–≤–æ–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —á–∞—Å—Ç–æ –µ—Å—Ç—å —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ batch['labels'] / batch['y'] / labels_pooled[...]
    if isinstance(batch, dict):
        for k in ["labels", "y", "target", "gt"]:
            if k in batch:
                return batch[k]
    # –µ—Å–ª–∏ batch = (inputs, labels) –∏–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂–µ–µ
    if isinstance(batch, (list, tuple)):
        for item in batch:
            if torch.is_tensor(item):
                # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏
                if item.dtype in (torch.int64, torch.int32, torch.uint8, torch.bool):
                    return item
    raise RuntimeError(
        "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–Ω–∑–æ—Ä –º–µ—Ç–æ–∫ –≤ batch ‚Äî –ø–æ–ø—Ä–∞–≤—å iter_labels_from_batch()."
    )


def compute_class_prior(dls, max_batches=None):
    total = 0
    pos = 0
    seen = 0
    for dl in dls:
        for i, batch in enumerate(dl):
            y = iter_labels_from_batch(batch)  # [B, H, N] / [B,N] / [B*N]
            y = y.detach()
            # –ø—Ä–∏–≤–µ–¥—ë–º –∫ [B*H*N]
            y_flat = y.view(-1).to(torch.float32)
            pos += y_flat.sum().item()
            total += y_flat.numel()
            seen += 1
            print(pos, total)
            if max_batches is not None and seen >= max_batches:
                break
    p1 = pos / (total + 1e-12)
    p0 = 1.0 - p1
    return p0, p1, int(total), int(pos)


if __name__ == "__main__":
    args = get_parser()

    check = args.meld_check
    wandb_logger = WandbLogger(project=args.project_name, log_model=True)

    tokenizer = AutoTokenizer.from_pretrained(args.bert_type, trust_remote_code=True)

    df = pd.read_csv(args.split_path, sep=",")

    train_ids = df[df.split == "trainval"]["subject_id"].tolist()
    # test_ids  = df[df.split=='test']['subject_id'].tolist()
    test_ids = df[(df["split"] == "test") & (df["subject_id"].str.contains("FCD"))][
        "subject_id"
    ].tolist()

    fold_metrics = []
    n_splits = 5
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)

    for fold, (train_index, val_index) in enumerate(kf.split(train_ids)):
        # if fold+1 < 4:
        #     continue
        print(f"Fold {fold + 1}/{n_splits}")

        train_fold_ids = [train_ids[i] for i in train_index]
        val_fold_ids = [train_ids[i] for i in val_index]

        print(f"Train IDs: {len(train_fold_ids)}")
        print(f"Validation IDs: {len(val_fold_ids)}")

        # 1. setting dataset and dataloader
        ds_train = EpilepDataset(
            csv_path=args.train_csv_path,
            root_path=args.train_root_path,
            tokenizer=tokenizer,
            mode="train",
            meld_path=args.meld_script_path,
            output_dir=args.output_dir,
            feature_path=args.feature_path,
            subject_ids=train_fold_ids,
            aug_flag=True,
        )

        ds_valid = EpilepDataset(
            csv_path=args.train_csv_path,
            root_path=args.train_root_path,
            tokenizer=tokenizer,
            mode="valid",
            meld_path=args.meld_script_path,
            output_dir=args.output_dir,
            feature_path=args.feature_path,
            subject_ids=val_fold_ids,
        )

        hc_set = set(
            [sid for sid in train_fold_ids if sid.split("_")[3].startswith("C")]
        )
        labels = [0 if sid in hc_set else 1 for sid in ds_train.subject_ids]

        sampler = LesionOversampleSampler(labels, seed=SEED)

        dl_train = DataLoader(
            ds_train,
            batch_size=args.train_batch_size,
            sampler=sampler,  # shuffle=True,
            num_workers=0,  # args.train_batch_size,
            pin_memory=True,
            worker_init_fn=worker_init_fn,
            # persistent_workers=True
        )

        dl_valid = DataLoader(
            ds_valid,
            batch_size=args.valid_batch_size,
            shuffle=False,
            num_workers=0,  # args.valid_batch_size,
            pin_memory=True,
            worker_init_fn=worker_init_fn,
            # persistent_workers=True
        )

        # –°—á–∏—Ç–∞–µ–º –Ω–∞ train (+ –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ val)
        # p0, p1, tot, pos = compute_class_prior([dl_train, dl_valid], max_batches=None)  # –∏–ª–∏ —Ç–æ–ª—å–∫–æ [dl_train]
        # print(f"[PRIOR] total={tot}, positives={pos}, p1={p1:.6f}, p0={p0:.6f}")
        # sys.exit(0)
        ## 2. setting trainer
        if torch.cuda.is_available():
            accelerator = "gpu"
            # TODO: Using more than 1 GPU require syncronization, because I got worse perfomance, than in 1 GPU
            devices = "auto"  # torch.cuda.device_count()
            strategy = "ddp_sharded"
        else:
            accelerator = "cpu"
            args.device = "cpu"
            devices = 1
            strategy = None

        ckpt_path = None
        print(f"[INFO] Loading model from checkpoint: {ckpt_path}")
        if ckpt_path is not None:
            model = LanGuideMedSegWrapper.load_from_checkpoint(
                checkpoint_path=ckpt_path,
                args=args,
                tokenizer=tokenizer,
                max_len=ds_valid.max_length,
                alpha=0.75,
                gamma=2,
            )
        else:
            model = LanGuideMedSegWrapper(
                args,
                tokenizer=tokenizer,
                max_len=ds_train.max_length,
                alpha=0.75,
                gamma=2,
            )

        ## 1. setting recall function
        model_ckpt = ModelCheckpoint(
            dirpath=args.model_save_path,
            filename=f"{args.model_save_filename}_fold{fold + 1}",
            monitor="val_dice",  # 'val_loss',
            save_top_k=1,
            mode="max",  # 'min',
            verbose=True,
        )

        # ckpt_loss = ModelCheckpoint(
        #     dirpath=args.model_save_path,
        #     filename=f"{args.model_save_filename}_fold{fold + 1}_best-loss",
        #     monitor="val_loss",
        #     save_top_k=1,
        #     mode="min",
        #     verbose=True,
        # )

        early_stopping = EarlyStopping(
            monitor="val_dice", #"val_loss",
            patience=args.patience,
            mode="max",  # 'min',
        )

        trainer = pl.Trainer(
            logger=wandb_logger,
            min_epochs=args.min_epochs,
            max_epochs=args.max_epochs,
            accelerator=accelerator,
            devices=devices,
            # strategy=strategy, ###################
            # callbacks=[ckpt_loss, model_ckpt, early_stopping],
            callbacks=[model_ckpt, early_stopping],
            enable_progress_bar=True,
            # precision=32,                  # –±–µ–∑ AMP
            # gradient_clip_val=1.0,         # —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç –≤—Å–ø–ª–µ—Å–∫–∏
            # accumulate_grad_batches=2,     # —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç —à—É–º (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
        )

        # 3. start training
        print("start training")
        trainer.fit(model, dl_train, dl_valid)  # , ckpt_path=ckpt_path)
        print("done training")

        # --- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ ---
        print("start testing")
        # mgh_cleaner()

        # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ DataLoader ---
        ds_test = EpilepDataset(
            csv_path=args.test_csv_path,  # –Ω–æ–≤—ã–π CSV –¥–ª—è —Ç–µ—Å—Ç–∞
            root_path=args.test_root_path,  # –ø—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
            tokenizer=tokenizer,
            mode="test",
            meld_path=args.meld_script_path,
            output_dir=args.output_dir,
            feature_path=args.feature_path,
            subject_ids=test_ids,
        )

        dl_test = DataLoader(
            ds_test,
            batch_size=args.valid_batch_size,
            shuffle=False,
            num_workers=2,
            # num_workers=1,
            pin_memory=True,
            worker_init_fn=worker_init_fn,
            persistent_workers=True,
        )

        # 3) Evaluate on TEST
        model.eval()

        # Checking MELD results on test data
        if check:
            test_metrics = nn.ModuleDict(
                {
                    "acc": Accuracy(task="binary"),
                    "dice": Dice(),
                    "ppv": Precision(task="binary"),
                    "MIoU": BinaryJaccardIndex(),
                }
            )
            cohort = MeldCohort()

            cortex_mask = torch.from_numpy(
                cohort.cortex_mask
            )  # shape [N], dtype=torch.bool

            metrics_history = {name: [] for name in test_metrics.keys()}

            for batch in dl_test:
                (subject_ids, text), y = batch
                B, H, N = y.shape

                for b, sid in enumerate(subject_ids):
                    # –∏–∑–≤–ª–µ–∫–∞–µ–º GT –¥–ª—è –∫–æ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã—Ö –≤–µ—Ä—à–∏–Ω
                    gt = y[b]  # [2, N]
                    gt_cortex = gt[:, cortex_mask]  # [2, N_cortex]
                    gt_flat = gt_cortex.flatten()  # [2*N_cortex]

                    # –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–∞—à–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    nii_path = os.path.join(
                        # MELD_DATA_PATH, f"input/{sid}/anat/features", "result.npz",
                        MELD_DATA_PATH,
                        f"preprocessed/meld_files/{sid}/features",
                        "result.npz",
                    )
                    arr = np.load(nii_path)["result"]
                    pred = torch.from_numpy(arr.astype("float32"))  # [2, N]

                    # pred_label = pred.argmax(dim=0)

                    # # 3) –±–∏–Ω–∞—Ä–∏–∑—É–µ–º: 1 ‚Äî –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç, 0 ‚Äî —à—É–º
                    # binary_pred = (pred_label == 1).cpu().numpy()  # ndarray [N_cortex]

                    # # 4) –Ω–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é)
                    # clusters, num_clusters = label(binary_pred)
                    # print(f"{sid} num_clusters = {num_clusters}")
                    # metrics_history.setdefault("num_clusters", []).append(num_clusters)

                    # # 6) –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É
                    # for cl_id in range(1, num_clusters + 1):
                    #     cluster_mask = clusters == cl_id
                    #     if cluster_mask.sum() == 0:
                    #         continue
                    #     for name, metric in test_metrics.items():
                    #         metric.update(
                    #             torch.from_numpy(cluster_mask).unsqueeze(
                    #                 0
                    #             ),  # [1, n_pts_total]
                    #             torch.from_numpy(
                    #                 gt_flat.reshape(2, -1)[1, :][cluster_mask]
                    #             ).unsqueeze(0),
                    #         )
                    #         val = metric.compute().item()
                    #         metric.reset()
                    #         print(f"{sid} {name} –∫–ª–∞—Å—Ç–µ—Ä {cl_id} = {val:.4f}")
                    #         key = f"{name}_cluster_{cl_id}"
                    #         metrics_history.setdefault(key, []).append(val)

                    # print("---")

                    # 3) –æ–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ history
                    for name, metric in test_metrics.items():
                        metric.update(pred, gt_flat.long())
                        val = metric.compute().item()
                        metric.reset()

                        print(f"{sid} {name} = {val:.4f}")
                        metrics_history[name].append(val)

                    print("---")

            # 4) –ø–æ—Å–ª–µ –≤—Å–µ—Ö —Å—É–±—ä–µ–∫—Ç–æ–≤ —Å—á–∏—Ç–∞–µ–º –º–µ–¥–∏–∞–Ω—É –∏ 2.5‚Äì97.5 –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏

            print("\n=== –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º —Å—É–±—ä–µ–∫—Ç–∞–º ===")
            for name, values in metrics_history.items():
                vals = np.array(values)
                med = np.median(vals)
                lo, hi = np.percentile(vals, [2.5, 97.5])
                print(f"{name:>5}: median={med:.4f}  [{lo:.4f}‚Äì{hi:.4f}]")
        
        test_results = trainer.test(
            model,
            dataloaders=dl_test,
            ckpt_path=ckpt_path,
            verbose=True,
        )
        print("=== TEST metrics ===")
        print(test_results)
        metrics = test_results[0]  # –æ–±—ã—á–Ω–æ —Ç–∞–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Ñ–æ–ª–¥–∞ –∫ –º–µ—Ç—Ä–∏–∫–∞–º
        metrics["fold"] = fold + 1
        fold_metrics.append(metrics)

        csv_output_path = os.path.join(args.output_dir, "kfold_metrics.csv")
        keys = fold_metrics[0].keys()
        with open(csv_output_path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(fold_metrics)

        print(f"üìä –û–±–Ω–æ–≤–ª—ë–Ω CSV —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ—Å–ª–µ —Ñ–æ–ª–¥–∞ {fold + 1}: {csv_output_path}")
        wandb.finish()
        break
