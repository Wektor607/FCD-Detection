import os
import torch
import argparse
import subprocess
import random
import csv
import numpy as np
import pandas as pd

import torch.multiprocessing
from torch.utils.data import DataLoader, Sampler

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger

from sklearn.model_selection import KFold
from transformers import AutoTokenizer

import wandb
import utils.config as config
from utils.data import EpilepDataset
from engine.wrapper import LanGuideMedSegWrapper
from meld_graph.paths import FS_SUBJECTS_PATH

# Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ñ‹Ð·Ð²Ð°Ñ‚ÑŒ
torch.multiprocessing.set_sharing_strategy("file_system")


SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ÐŸÐ¾Ð»Ð½Ð°Ñ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ TF32 (Ð´Ð°Ñ‘Ñ‚ Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð¹ Ð´Ñ€ÐµÐ¹Ñ„ Ð½Ð° Ampere/Ada)
torch.use_deterministic_algorithms(True, warn_only=True)
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
# (Ð¾Ð¿Ñ†.) Ð´Ð»Ñ ÐµÐ´Ð¸Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ð¾Ð¹ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ‡Ð½Ð¾Ð¹ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸
# torch.set_float32_matmul_precision("high")


# Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚Ðµ DataLoader Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼Ð¸ Ð²Ð¾Ñ€ÐºÐµÑ€Ð°Ð¼Ð¸:
def worker_init_fn(worker_id):
    np.random.seed(SEED + worker_id)
    random.seed(SEED + worker_id)


def get_parser():
    parser = argparse.ArgumentParser(
        description="Language-guide Medical Image Segmentation"
    )
    parser.add_argument(
        "--config", default="./config/training.yaml", type=str, help="config file"
    )
    parser.add_argument("--meld_check", default=False, type=bool, help="config file")

    args = parser.parse_args()
    assert args.config is not None
    cfg = config.load_cfg_from_cfg_file(args.config)
    cfg.meld_check = args.meld_check
    return cfg


def mgh_cleaner():
    cleanup_cmd = [
        "find",
        os.path.join(FS_SUBJECTS_PATH),
        "-type",
        "f",
        "-path",
        "*/xhemi/classifier/*",
        "-delete",
    ]
    subprocess.run(cleanup_cmd, check=True)


class LesionOversampleSampler(Sampler):
    """
    Ð¡ÑÐ¼Ð¿Ð»ÐµÑ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð±ÐµÑ€Ñ‘Ñ‚ Ð’Ð¡Ð• healthy-Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ñ€Ð¾Ð²Ð½Ð¾ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ñ€Ð°Ð·Ñƒ,
    Ð° lesion-Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ â€” Ñ replacement, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð²ÑÑŽ ÑÐ¿Ð¾Ñ…Ñƒ.
    """

    def __init__(self, labels, seed=42):
        self.labels = labels
        random.seed(seed)
        # Ð¸Ð½Ð´ÐµÐºÑÑ‹ Ð·Ð´Ð¾Ñ€Ð¾Ð²Ñ‹Ñ… Ð¸ lesion
        self.hc_idx = [i for i, label in enumerate(labels) if label == 0]
        self.les_idx = [i for i, label in enumerate(labels) if label == 1]
        # Ñ…Ð¾Ñ‚Ð¸Ð¼ Ñ€Ð¾Ð²Ð½Ð¾ len(labels) Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ðº Ð·Ð° ÑÐ¿oÑ…Ñƒ
        self.epoch_size = len(labels)

    def __iter__(self):
        # Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ñ Ð²ÑÐµÑ… hc-Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²
        idxs = self.hc_idx.copy()
        # ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð´Ð¾ÐºÐ¸Ð½ÑƒÑ‚ÑŒ lesion'Ð¾Ð²
        n_les_to_sample = self.epoch_size - len(idxs)
        # Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ lesion Ñ replacement
        idxs += random.choices(self.les_idx, k=n_les_to_sample)
        # Ð¿ÐµÑ€ÐµÐ¼ÐµÑˆÐ¸Ð²Ð°ÐµÐ¼ Ð²ÑÑŽ ÑÐ¿Ð¾Ñ…Ñƒ
        random.shuffle(idxs)
        return iter(idxs)

    def __len__(self):
        return self.epoch_size


def iter_labels_from_batch(batch):
    """
    ÐŸÑ€Ð¸Ð²ÐµÐ´Ð¸ Ðº ÑÐ²Ð¾ÐµÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ: Ð²ÐµÑ€Ð½Ð¸ Ñ‚ÐµÐ½Ð·Ð¾Ñ€ Ð¼ÐµÑ‚Ð¾Ðº Ð½Ð° Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼ ÑƒÑ€Ð¾Ð²Ð½Ðµ.
    Ð”Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ bool/0-1, shape [B, H, N] Ð¸Ð»Ð¸ [B, N] Ð¸Ð»Ð¸ [B*N].
    """
    # Ð¿Ñ€Ð¸Ð¼ÐµÑ€ â€” Ð¿Ð¾Ð´ÑÑ‚Ñ€Ð¾Ð¹ Ð¿Ð¾Ð´ ÑÐ²Ð¾Ð¹ batch
    # Ð¸Ð· Ñ‚Ð²Ð¾ÐµÐ³Ð¾ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¾ ÐµÑÑ‚ÑŒ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð²Ñ€Ð¾Ð´Ðµ batch['labels'] / batch['y'] / labels_pooled[...]
    if isinstance(batch, dict):
        for k in ["labels", "y", "target", "gt"]:
            if k in batch:
                return batch[k]
    # ÐµÑÐ»Ð¸ batch = (inputs, labels) Ð¸Ð»Ð¸ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð¿Ð¾Ñ…Ð¾Ð¶ÐµÐµ
    if isinstance(batch, (list, tuple)):
        for item in batch:
            if torch.is_tensor(item):
                # ÑÐ²Ñ€Ð¸ÑÑ‚Ð¸ÐºÐ°: Ð±Ð¸Ð½Ð°Ñ€Ð½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸
                if item.dtype in (torch.int64, torch.int32, torch.uint8, torch.bool):
                    return item
    raise RuntimeError(
        "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ñ‚ÐµÐ½Ð·Ð¾Ñ€ Ð¼ÐµÑ‚Ð¾Ðº Ð² batch â€” Ð¿Ð¾Ð¿Ñ€Ð°Ð²ÑŒ iter_labels_from_batch()."
    )


def compute_class_prior(dls, max_batches=None):
    total = 0
    pos = 0
    seen = 0
    for dl in dls:
        for i, batch in enumerate(dl):
            y = iter_labels_from_batch(batch)  # [B, H, N] / [B,N] / [B*N]
            y = y.detach()
            # Ð¿Ñ€Ð¸Ð²ÐµÐ´Ñ‘Ð¼ Ðº [B*H*N]
            y_flat = y.view(-1).to(torch.float32)
            pos += y_flat.sum().item()
            total += y_flat.numel()
            seen += 1
            print(pos, total)
            if max_batches is not None and seen >= max_batches:
                break
    p1 = pos / (total + 1e-12)
    p0 = 1.0 - p1
    return p0, p1, int(total), int(pos)


if __name__ == "__main__":
    args = get_parser()

    check = args.meld_check
    wandb_logger = WandbLogger(project=args.project_name, log_model=True)

    tokenizer = AutoTokenizer.from_pretrained(args.bert_type, trust_remote_code=True)

    df = pd.read_csv(args.split_path, sep=",")

    train_ids = df[df.split == "trainval"]["subject_id"].tolist()
    # test_ids  = df[df.split=='test']['subject_id'].tolist()
    test_ids = df[(df["split"] == "test") & (df["subject_id"].str.contains("FCD"))][
        "subject_id"
    ].tolist()

    fold_metrics = []
    n_splits = 5
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)

    for fold, (train_index, val_index) in enumerate(kf.split(train_ids)):
        # if fold+1 < 4:
        #     continue
        print(f"Fold {fold + 1}/{n_splits}")

        train_fold_ids = [train_ids[i] for i in train_index]
        val_fold_ids = [train_ids[i] for i in val_index]

        print(f"Train IDs: {len(train_fold_ids)}")
        print(f"Validation IDs: {len(val_fold_ids)}")

        # 1. setting dataset and dataloader
        ds_train = EpilepDataset(
            csv_path=args.train_csv_path,
            root_path=args.train_root_path,
            tokenizer=tokenizer,
            mode="train",
            meld_path=args.meld_script_path,
            output_dir=args.output_dir,
            feature_path=args.feature_path,
            subject_ids=train_fold_ids,
            aug_flag=True,
        )

        ds_valid = EpilepDataset(
            csv_path=args.train_csv_path,
            root_path=args.train_root_path,
            tokenizer=tokenizer,
            mode="valid",
            meld_path=args.meld_script_path,
            output_dir=args.output_dir,
            feature_path=args.feature_path,
            subject_ids=val_fold_ids,
        )

        hc_set = set(
            [sid for sid in train_fold_ids if sid.split("_")[3].startswith("C")]
        )
        labels = [0 if sid in hc_set else 1 for sid in ds_train.subject_ids]

        sampler = LesionOversampleSampler(labels, seed=SEED)

        dl_train = DataLoader(
            ds_train,
            batch_size=args.train_batch_size,
            sampler=sampler,  # shuffle=True,
            num_workers=0,  # args.train_batch_size,
            pin_memory=True,
            worker_init_fn=worker_init_fn,
            # persistent_workers=True
        )

        dl_valid = DataLoader(
            ds_valid,
            batch_size=args.valid_batch_size,
            shuffle=False,
            num_workers=0,  # args.valid_batch_size,
            pin_memory=True,
            worker_init_fn=worker_init_fn,
            # persistent_workers=True
        )

        # Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð½Ð° train (+ Ð¿Ñ€Ð¸ Ð¶ÐµÐ»Ð°Ð½Ð¸Ð¸ val)
        # p0, p1, tot, pos = compute_class_prior([dl_train, dl_valid], max_batches=None)  # Ð¸Ð»Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ [dl_train]
        # print(f"[PRIOR] total={tot}, positives={pos}, p1={p1:.6f}, p0={p0:.6f}")
        # sys.exit(0)
        ## 2. setting trainer
        if torch.cuda.is_available():
            accelerator = "gpu"
            # TODO: Using more than 1 GPU require syncronization, because I got worse perfomance, than in 1 GPU
            devices = "auto"  # torch.cuda.device_count()
            strategy = "ddp_sharded"
        else:
            accelerator = "cpu"
            args.device = "cpu"
            devices = 1
            strategy = None

        ckpt_path = None
        print(f"[INFO] Loading model from checkpoint: {ckpt_path}")
        if ckpt_path is not None:
            model = LanGuideMedSegWrapper.load_from_checkpoint(
                checkpoint_path=ckpt_path,
                args=args,
                tokenizer=tokenizer,
                max_len=ds_valid.max_length,
                alpha=0.75,
                gamma=2,
            )
        else:
            model = LanGuideMedSegWrapper(
                args,
                tokenizer=tokenizer,
                max_len=ds_train.max_length,
                alpha=0.75,
                gamma=2,
            )

        ## 1. setting recall function
        model_ckpt = ModelCheckpoint(
            dirpath=args.model_save_path,
            filename=f"{args.model_save_filename}_fold{fold + 1}",
            monitor="val_dice",  # 'val_loss',
            save_top_k=1,
            mode="max",  # 'min',
            verbose=True,
        )

        early_stopping = EarlyStopping(
            monitor="val_dice",  # 'val_loss',
            patience=args.patience,
            mode="max",  # 'min',
        )

        trainer = pl.Trainer(
            logger=wandb_logger,
            min_epochs=args.min_epochs,
            max_epochs=args.max_epochs,
            accelerator=accelerator,
            devices=devices,
            # strategy=strategy, ###################
            callbacks=[model_ckpt, early_stopping],
            enable_progress_bar=True,
            # precision=32,                  # Ð±ÐµÐ· AMP
            # gradient_clip_val=1.0,         # ÑÐ³Ð»Ð°Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð²ÑÐ¿Ð»ÐµÑÐºÐ¸
            # accumulate_grad_batches=2,     # ÑÐ³Ð»Ð°Ð¶Ð¸Ð²Ð°ÐµÑ‚ ÑˆÑƒÐ¼ (Ð¿Ð¾ Ð¶ÐµÐ»Ð°Ð½Ð¸ÑŽ)
        )

        # 3. start training
        print("start training")
        trainer.fit(model, dl_train, dl_valid)  # , ckpt_path=ckpt_path)
        print("done training")

        # --- Ð—Ð°Ð¿ÑƒÑÐº Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð³Ð¾Ð½Ð° ---
        print("start testing")
        # mgh_cleaner()

        # --- ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ Ð¸ DataLoader ---
        ds_test = EpilepDataset(
            csv_path=args.test_csv_path,  # Ð½Ð¾Ð²Ñ‹Ð¹ CSV Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð°
            root_path=args.test_root_path,  # Ð¿ÑƒÑ‚ÑŒ Ðº Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼
            tokenizer=tokenizer,
            mode="test",
            meld_path=args.meld_script_path,
            output_dir=args.output_dir,
            feature_path=args.feature_path,
            subject_ids=test_ids,
        )

        dl_test = DataLoader(
            ds_test,
            batch_size=args.valid_batch_size,
            shuffle=False,
            num_workers=2,
            # num_workers=1,
            pin_memory=True,
            worker_init_fn=worker_init_fn,
            persistent_workers=True,
        )

        # 3) Evaluate on TEST
        model.eval()
        test_results = trainer.test(
            model,
            dataloaders=dl_test,
            ckpt_path=ckpt_path,
            verbose=True,
        )
        print("=== TEST metrics ===")
        print(test_results)
        metrics = test_results[0]  # Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ñ‚Ð°Ð¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð´Ð¸Ð½ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð¾Ð¼ÐµÑ€ Ñ„Ð¾Ð»Ð´Ð° Ðº Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼
        metrics["fold"] = fold + 1
        fold_metrics.append(metrics)

        csv_output_path = os.path.join(args.output_dir, "kfold_metrics.csv")
        keys = fold_metrics[0].keys()
        with open(csv_output_path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(fold_metrics)

        print(f"ðŸ“Š ÐžÐ±Ð½Ð¾Ð²Ð»Ñ‘Ð½ CSV Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¾Ð»Ð´Ð° {fold + 1}: {csv_output_path}")
        wandb.finish()
        break
